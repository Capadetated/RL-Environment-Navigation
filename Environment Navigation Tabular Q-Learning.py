# -*- coding: utf-8 -*-
"""Q-Learning Factory A5b: Q9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1klxJJr3zSyyx2-UzZQzP4Tg9DIZw-7BG
"""

# Code based upon work by Sayak, P. (2019). FloydHub-Q-Learning-Blog. GitHub. https://github.com/sayakpaul/FloydHub-Q-Learning-Blog
# Generated for DATA 640 for Dr. Steven Knode by Theodore Fitch
# Last updated 26MAR24

# Import only numpy
import numpy as np

# Initialize parameters
gamma = 0.9 # Discount factor. Adjusted to 0.05 to observe effects.
alpha = 0.75 # Learning rate. Adjusted to 0.05 to observe effects.

# Define the states. 10th state added
location_to_state = {
    'L1' : 0,
    'L2' : 1,
    'L3' : 2,
    'L4' : 3,
    'L5' : 4,
    'L6' : 5,
    'L7' : 6,
    'L8' : 7,
    'L9' : 8,
    'L10' : 9
}

# Define the actions. 10th action added
actions = [0,1,2,3,4,5,6,7,8,9]

# Define the rewards. A 10th column and row added. L10 is only accessible via L9
rewards = np.array([[0,1,0,0,0,0,0,0,0,0],
              [1,0,1,0,1,0,0,0,0,0],
              [0,1,0,0,0,1,0,0,0,0],
              [0,0,0,0,0,0,1,0,0,0],
              [0,1,0,0,0,0,0,1,0,0],
              [0,0,1,0,0,0,0,0,0,0],
              [0,0,0,1,0,0,0,1,0,0],
              [0,0,0,0,1,0,1,0,1,0],
              [0,0,0,0,0,0,0,1,0,1],
              [0,0,0,0,0,0,0,0,1,0]])

# Maps indices to locations
state_to_location = dict((state,location) for location,state in location_to_state.items())

def get_optimal_route(start_location,end_location):
    # Copy the rewards matrix to new Matrix
    rewards_new = np.copy(rewards)
    # Get the ending state corresponding to the ending location as given
    ending_state = location_to_state[end_location]
    # With the above information automatically set the priority of the given ending state to the highest one
    rewards_new[ending_state,ending_state] = 999

    # -----------Q-Learning algorithm-----------

    # Initializing Q-Values. Array expanded to 10x10
    Q = np.array(np.zeros([10,10]))

    # Q-Learning process
    for i in range(1000):
        # Pick up a state randomly
        current_state = np.random.randint(0,10) # Python excludes the upper bound
        # For traversing through the neighbor locations in the maze
        playable_actions = []
        # Iterate through the new rewards matrix and get the actions > 0
        for j in range(9):
            if rewards_new[current_state,j] > 0:
                playable_actions.append(j)
        # Pick an action randomly from the list of playable actions leading us to the next state
        next_state = np.random.choice(playable_actions)
        # Compute the temporal difference
        # The action here exactly refers to going to the next state
        TD = rewards_new[current_state,next_state] + gamma * Q[next_state, np.argmax(Q[next_state,])] - Q[current_state,next_state]
        # Update the Q-Value using the Bellman equation
        Q[current_state,next_state] += alpha * TD

    # Initialize the optimal route with the starting location
    route = [start_location]
    # We do not know about the next location yet, so initialize with the value of starting location
    next_location = start_location
    # Editing while loop to show iterations
    steps=0
    # We don't know about the exact number of iterations needed to reach to the final location hence while loop will be a good choice for iteratiing
    while(next_location != end_location):
        # Fetch the starting state
        starting_state = location_to_state[start_location]
        # Fetch the highest Q-value pertaining to starting state
        next_state = np.argmax(Q[starting_state,])
        # We got the index of the next state. But we need the corresponding letter.
        next_location = state_to_location[next_state]
        route.append(next_location)
        # Update the starting location for the next iteration
        start_location = next_location
        # Upversion steps
        steps=steps+1
     # Rounding off the Q-values
    Q = np.round(Q)
    # For question 9, alternate title L1/L4
    print("1,000 Iterations; L10 to L4")
    print(Q)
    return route,steps

# For question 9, this is alternated from L10 to L1, and L10 to L4
print(get_optimal_route('L10', 'L4'))